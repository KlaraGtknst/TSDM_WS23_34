{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from copy import deepcopy\n",
    "import sys\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Daily Temperature Forecasts\n",
    "In this task you need to build sequence-to-sequence model for temperature forecast using RNN and LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Import daily temperature dataset **daily-minimum-temperatures.csv** in the data folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data = pd.read_csv('./data/daily-minimum-temperatures.csv',header=0, index_col=0)\n",
    "temp_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Scale the training and test data set using MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_length = 50\n",
    "train, test = temp_data[:-test_length], temp_data[-test_length:]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "# Scaling the train and test data using minmaxscaler\n",
    "### Your Code Here ### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Build sequence-to-sequence data. For example, the length of the input and target sequence is 4 time steps, i.e.:\n",
    "\n",
    "        The raw data: [1,2,3,4,5,6,7,8,9,10,...]\n",
    "\n",
    "                              Prediction\n",
    "        Input Sequence      -------------->      Target Sequence\n",
    "          [1,2,3,4]                                [5,6,7,8]\n",
    "          [2,3,4,5]                                [6,7,8,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_2_seq(data, length_x, length_y):\n",
    "    '''\n",
    "    convert the input data to seq-to-seq samples for training rnn model.\n",
    "    \n",
    "    Inputs:\n",
    "    data: pd.DataFrame, the input temperature data set.\n",
    "    length_x: The length of the input sequence\n",
    "    length_y: The length of the output sequence\n",
    "    \n",
    "    Outputs:\n",
    "    X: Input sequence, the size is: [number of samples, length_x, features]\n",
    "    y: Output sequence, the size is: [number of samples, length_y, features]\n",
    "    '''\n",
    "    ### Your Code Here ###\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input sequence size:  (num_sample, 7, 1)\n",
    "# Target sequence size: (num_sample, 1, 1)\n",
    "train_X, train_y = seq_2_seq(train_scaled, 7, 1)\n",
    "test_X, test_y = seq_2_seq(test_scaled, 7, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Note that the NaN in the last rows should be dropped if you use pd.DataFrame.shift().\n",
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Note that the NaN in the last rows should be dropped if you use pd.DataFrame.shift().\n",
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the NaN in the last rows should be dropped if you use pd.DataFrame.shift().\n",
    "test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Note that the NaN in the last rows should be dropped if you use pd.DataFrame.shift().\n",
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the sequneces to tensor (dtype=torch.float32) with a size (num_sample, sequence_length, feature_size). \n",
    "### Your Code Here ### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** Complete the class of plain Vanilla Recurrent Neural Networks.\n",
    "\n",
    "The model contains one [RNN Layer](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html), which is followed by a Linear layer.\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "- The size of input data: ($L,N,H_{in}$), or ($N,L,H_{in}$) when batch_first=True.\n",
    "\n",
    "  $N$ = batch size\n",
    "  \n",
    "  $L$ = sequence length\n",
    "  \n",
    "  $H_{in}$ = input size\n",
    "  \n",
    "- The size of the connected linear layer: ($H_{rnn-out}, H_{out}$)\n",
    "    \n",
    "  $H_{rnn-out}$ = hidden size of the rnn layer\n",
    "  \n",
    "  $H_{out}$ = output size of the linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rnn(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size=1, output_size=1, hidden_size=32, num_layers=1, dropout=0.2,\n",
    "                 seq_length_in = 7, seq_length_out = 1, batch_first=True, \n",
    "                 activation_function='relu', random_init=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        '''\n",
    "        Initialize the hyperparameters and the layers of the network, including rnn layers and a linear layer.\n",
    "        input_size: int, dimension of the input features at each time step\n",
    "        output_size: int, dimension of the target at each time step\n",
    "        hidden_size: number of neuros in rnn layers\n",
    "        num_layers: number of rnn layers\n",
    "        dropout: dropout rate in rnn layers\n",
    "        seq_length_in: the sequence length of input data\n",
    "        seq_length_out: the sequence length of output (target) data\n",
    "        batch_first: boolean, if true, the size of the input data should be: \n",
    "                     [batch_size (number of samples), sequence length, features(or targets)]\n",
    "        activation_function: str, activation function in rnn layers\n",
    "        random_init: boolean, if true, the initial hidden states of the rnn layers are sampled from a normal distribution;\n",
    "                     Otherwise, it is set to zero. \n",
    "        '''\n",
    "        ### Your Code Here ###\n",
    "        pass\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: torch.tensor, the input\n",
    "        \n",
    "        Feedforward propagation of the input to the output.\n",
    "        '''\n",
    "        \n",
    "        if self.batch_first:\n",
    "            # get the batch size from the input x to create the initial hidden states             \n",
    "            ### Your Code Here ### \n",
    "        else:\n",
    "            ### Your Code Here ###\n",
    "            pass\n",
    "        if self.random_init:\n",
    "            # Create the initial hidden states\n",
    "            ### Your Code Here ###\n",
    "            pass\n",
    "        else:\n",
    "            ### Your Code Here ###\n",
    "            pass\n",
    "        \n",
    "        # feedforward propagation of the input to the output        \n",
    "        ### Your Code Here ###\n",
    "        pass\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rnn()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e)** Complete the function **training_process()** and train the rnn model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The hyperparameters\n",
    "epochs = 256\n",
    "hidden_size = 32\n",
    "num_layers = 2\n",
    "input_size = train_X_tensor.shape[2]\n",
    "seq_length_in = train_X_tensor.shape[1]\n",
    "output_size = train_y_tensor.shape[2]\n",
    "seq_length_out = train_y_tensor.shape[1]\n",
    "activation_function = 'relu'\n",
    "dropout_rate = 0.2\n",
    "batch_size = 128\n",
    "early_stop_patience = 30\n",
    "batch_first = True\n",
    "random_init = True\n",
    "\n",
    "# Dataloader\n",
    "dataset = TensorDataset(train_X_tensor, train_y_tensor)\n",
    "loader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "model = rnn(input_size=input_size, output_size=output_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "            dropout=dropout_rate, seq_length_in = seq_length_in, seq_length_out = seq_length_out,\n",
    "            batch_first=batch_first, activation_function=activation_function, random_init=random_init)\n",
    "\n",
    "# Define the MSE loss function\n",
    "### Your Code Here ###\n",
    "pass\n",
    "\n",
    "# Define the adam optimizer\n",
    "### Your Code Here ###\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_process(model, epochs, loader, optimizer, test_X_tensor, test_y_tensor, early_stop_patience):\n",
    "    '''\n",
    "    The training process is monitored by early stopping\n",
    "    \n",
    "    Inputs:\n",
    "    model: the neural network object of the given class\n",
    "    epochs: training iterations\n",
    "    loader: data loader for mini-batch training\n",
    "    optimizer: the given optimizer\n",
    "    test_X_tensor: torch.tensor, the input sequneces of test data\n",
    "    test_y_tensor: torcg.tensor, the target sequences of test data\n",
    "    early_stop_patience: int, the epochs for stopping the training\n",
    "    \n",
    "    Outputs:\n",
    "    best_model: the model with the lowest test error. Use deepcopy() to copy the best one.\n",
    "    train_losses: training losses\n",
    "    test_losses: test losses\n",
    "    '''\n",
    "    train_losses = [] # save the average training loss of each iteration\n",
    "    test_losses = [] # save the average test loss of each iteration\n",
    "    best_model = deepcopy(model)\n",
    "    \n",
    "    best_epoch = 0 # the epoch of the lowest test loss\n",
    "    lowest_loss = sys.float_info.max # the current lowest test loss\n",
    "    stop_epochs = 0\n",
    "    \n",
    "    # Training process\n",
    "    for epoch in range(epochs):\n",
    "        # set training mode\n",
    "        model.train()\n",
    "        running_losses = 0.0\n",
    "\n",
    "        # Implement the training loop using mini-batch \n",
    "        for idx, batch in enumerate(loader):\n",
    "            ### Your Code Here ###\n",
    "            pass\n",
    "\n",
    "        # compute test error in each epoch and add it to test_losses\n",
    "        # Do the same for training loss as well\n",
    "        # print the test error in each 10 epochs\n",
    "        ### Your Code Here ###\n",
    "        pass\n",
    "        \n",
    "        # Implment early stopping\n",
    "        # Update the lowest loss, when the newest test loss is smaller.\n",
    "        # Otherwise stop_epochs += 1\n",
    "        # Stop learning and return the lowest test loss, when the stop_epochs is greater than or equal to\n",
    "        # the early_stop_patience\n",
    "        ### Your Code Here ###\n",
    "        pass\n",
    "        \n",
    "    return best_model, train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, train_losses, test_losses = training_process(model, epochs, loader, optimizer, \n",
    "                                                    test_X_tensor, test_y_tensor, early_stop_patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curve\n",
    "### Your Code Here ###\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the comparison between measurements and predictions of the training data\n",
    "### Your Code Here ###\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the comparison between measurements and predictions of the test data\n",
    "### Your Code Here ###\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Wind Power Generation Forecasts using LSTM (Bonus)\n",
    "\n",
    "In this exercise you need to implement an LSTM model to forecast wind power generation in the specific period based on the given 7 weather features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Import wind farm time-series dataset **wf.hdf5** in the data folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore = pd.HDFStore('./data/wf.hdf5')\n",
    "data = datastore.get('windfarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The windfarm dataset contains **hourly averaged wind power generation time series** for two consecutive years and **the corresponding day-ahead meteorological forecasts** using the European Centre for Medium-Range Weather Forecasts (ECMWF) weather model.\n",
    "\n",
    "The data set contains the following data items:\n",
    "\n",
    "- Time Stamp of the measurement\n",
    "- Forecasting Time, Time between the creation of the forecast to the forecasted point in time\n",
    "- Air Pressure of the measurement\n",
    "- Air Temperature of the measurement\n",
    "- Humidity\n",
    "- Wind Speed in 100m height\n",
    "- Wind Speed in 10m height\n",
    "- Wind Direction (zonal) in 100m height\n",
    "- Wind Direction (meridional) in 100m height\n",
    "- Wind Power Generation of the wind farm\n",
    "\n",
    "**The power generation time series are normalized** with the respective nominal capacity of the wind farm in order to enable a scale-free comparison and to mask the original characteristics of the wind farm. \n",
    "\n",
    "Additionally, **all weather situations are normalized in the range \\[0..1\\]**. \n",
    "\n",
    "The data set is pre-filtered to **discard any period of time longer than 24h in which no energy has been produced**, as this is an indicator of a wind farm malfunction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Visualize the weather features and the power generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data.columns[2:]:\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(data[col].values[:1000])\n",
    "    plt.title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Convert the wind farm time series to sequence-to-sequence training samples.\n",
    "        \n",
    "**Hints**:\n",
    "\n",
    "- The dimension of input sequences should be $[N, 6, 7]$, where $N$ indicates the number of samples, 6 indicates the time steps of the input sequence, and 7 indicates the dimension of weather features.\n",
    "\n",
    "- The dimension of an output sequnece should be $[N, 6, 1]$, where $N$ indicates the number of samples, 6 indicates the time steps of the target sequence, and 1 indicates the dimension of the target, i.e., wind power generation.\n",
    "\n",
    "- The **discarded periods** should be considered while building sequence-to-squence samples. If the difference between the two successive time indices is greater than 1 hour, **print** and **store** the two successive time indices.\n",
    "\n",
    "- You can compare your cleaned sequence data to the given data in 'clean_wf.hdf5'.\n",
    "\n",
    "- If you have no idea how to implement the function **seq_2_seq**, you can directly use 'clean_wf.hdf5' to finish the following tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = data[:12000], data[12000:]\n",
    "seq_length = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the discarded periods in training dataset \n",
    "discarded_periods_train = []\n",
    "discarded_periods_train.append(train.index[0])\n",
    "print('discarded periods in training dataset:\\n')\n",
    "for idx_1, idx_0 in zip(train.index[1:], train.index[:-1]):\n",
    "    # if the difference between two successive time indices is greater than 1 hour, print and store the time indices.     \n",
    "    ### Your Code Here ###\n",
    "    pass\n",
    "    \n",
    "discarded_periods_train.append(train.index[-1])\n",
    "\n",
    "\n",
    "# Print the discarded periods in test dataset \n",
    "discarded_periods_test = []\n",
    "discarded_periods_test.append(test.index[0])\n",
    "print('\\ndiscarded periods in test dataset:\\n')\n",
    "for idx_1, idx_0 in zip(test.index[1:], test.index[:-1]):\n",
    "    # if the difference between two successive time indices is greater than 1 hour, print and store the time indices.\n",
    "    ### Your Code Here ###\n",
    "    pass\n",
    "    \n",
    "discarded_periods_test.append(test.index[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_2_seq(data, discarded_periods, seq_length):\n",
    "    '''\n",
    "    Clean the discareded periods and convert the data to sequence-to-sequence data\n",
    "    \n",
    "    Inputs:\n",
    "    data: pd.DataFrame, the raw data\n",
    "    discareded_periods: list, which contains the interrupted time indices.\n",
    "    seq_length: int, the sequence length of generated input and output data.\n",
    "    \n",
    "    '''\n",
    "    data_seq2seq = pd.DataFrame([],columns=data.columns)\n",
    "    for i in range(0,len(discarded_periods), 2):\n",
    "        data_selected = data.loc[discarded_periods[i]:discarded_periods[i+1]]\n",
    "        if data_selected.shape[0] < seq_length:\n",
    "            continue\n",
    "        \n",
    "        for idx in range(data_selected.shape[0]-seq_length):\n",
    "            # concatenate the successive samples with the given length to the data_seq2seq\n",
    "            ### Your Code Here ###\n",
    "            pass\n",
    " \n",
    "    return data_seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = seq_2_seq(train, discarded_periods_train, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq.iloc[:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = seq_2_seq(test, discarded_periods_test, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq.iloc[:14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can compare your cleaned sequence data to the given data in 'clean_wf.hdf5'.\n",
    "\n",
    "If you have no idea how to implement the function **seq_2_seq**, you can directly use 'clean_wf.hdf5' to finish the following tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the clean_wd,hdf5 if you can not implement the function seq_2_seq\n",
    "# Otherwise, use the function seq_2_seq to generate seq-seq samples for training and test.\n",
    "### Your Code Here ###\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train_seq/test_seq to train_X and train_y/test_X and test_y with the size \n",
    "# [number of samples, sequence length, features(target)], and convert to torch.tensor (dtype=torch.float32).\n",
    "### Your Code Here ###\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'size of train_X_tensor: {train_X_tensor.shape}')\n",
    "train_X_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'size of test_X_tensor: {test_X_tensor.shape}')\n",
    "test_X_tensor[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** Complete the class of LSTM models.\n",
    "\n",
    "The model contains [LSTM Layer](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html), which is followed by a Linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size=1, output_size=1, hidden_size=32, num_layers=1, dropout=0.2,\n",
    "                 seq_length_in = 7, seq_length_out = 1, batch_first=True, random_init=True):\n",
    "        super().__init__()\n",
    "        '''\n",
    "        Initialize the hyperparameters and the layers of the network, including lstm layers and a linear layer.\n",
    "        input_size: int, dimension of the input features at each time step\n",
    "        output_size: int, dimension of the target at each time step\n",
    "        hidden_size: number of neuros in lstm layers\n",
    "        num_layers: number of lstm layers\n",
    "        dropout: dropout rate in lstm layers\n",
    "        seq_length_in: the sequence length of input data\n",
    "        seq_length_out: the sequence length of output (target) data\n",
    "        batch_first: boolean, if true, the size of the input data should be: \n",
    "                     [batch_size (number of samples), sequence length, features(or targets)]\n",
    "        random_init: boolean, if true, the initial hidden states and cell states of the lstm layers are sampled from a normal \n",
    "                     distribution; Otherwise, it is set to zero. \n",
    "        '''\n",
    "        ### Your Code Here ###\n",
    "        pass\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: torch.tensor, the input\n",
    "        \n",
    "        Feedforward propagation of the input to the output.\n",
    "        '''\n",
    "        \n",
    "        if self.batch_first:\n",
    "            # get the batch size from the input x to create the initial hidden states h0 and cell states c0\n",
    "            ### Your Code Here ###\n",
    "            pass\n",
    "        else:\n",
    "            ### Your Code Here ###\n",
    "            pass\n",
    "        if self.random_init:\n",
    "            ### Your Code Here ###\n",
    "            pass\n",
    "        else:\n",
    "            ### Your Code Here ###\n",
    "            pass\n",
    "        \n",
    "        # feedforward propagation of the input to the output \n",
    "        ### Your Code Here ###\n",
    "        pass\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e)** Declare an lstm model and train it using the above function **training_process()**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The hyperparameters\n",
    "epochs = 256\n",
    "hidden_size = 32\n",
    "num_layers = 2\n",
    "input_size = train_X_tensor.shape[2]\n",
    "seq_length_in = train_X_tensor.shape[1]\n",
    "output_size = train_y_tensor.shape[2]\n",
    "seq_length_out = train_y_tensor.shape[1]\n",
    "dropout_rate = 0.2\n",
    "batch_size = 128\n",
    "early_stop_patience = 30\n",
    "batch_first = True\n",
    "random_init = True\n",
    "\n",
    "# Dataloader\n",
    "dataset = TensorDataset(train_X_tensor, train_y_tensor)\n",
    "loader = DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = lstm(input_size=input_size, output_size=output_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                  dropout=dropout_rate, seq_length_in = seq_length_in, seq_length_out = seq_length_out,\n",
    "                  batch_first=batch_first, random_init=random_init)\n",
    "\n",
    "\n",
    "\n",
    "# Define loss function\n",
    "### Your Code Here ###\n",
    "pass\n",
    "\n",
    "# Define optimizer\n",
    "### Your Code Here ###\n",
    "pass\n",
    "\n",
    "# Train the lstm model using the function training_process()\n",
    "lstm_model, train_losses, test_losses = training_process(lstm_model, epochs, loader, optimizer_lstm, \n",
    "                                                         test_X_tensor, test_y_tensor, early_stop_patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the comparison between measurements and predictions of the test data at the first time step, \n",
    "# i.e, plt.plot(test_y_tensor[:,0,:])\n",
    "### Your Code Here ###\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the error of each time step. \n",
    "### Your Code Here ###\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
