{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from copy import deepcopy\n",
    "import sys\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Daily Temperature Forecasts\n",
    "In this task you need to build sequence-to-sequence model for temperature forecast using RNN and LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Import daily temperature dataset **daily-minimum-temperatures.csv** in the data folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1981-01-01</th>\n",
       "      <td>20.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-02</th>\n",
       "      <td>17.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-03</th>\n",
       "      <td>18.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-04</th>\n",
       "      <td>14.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-05</th>\n",
       "      <td>15.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-06</th>\n",
       "      <td>15.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-07</th>\n",
       "      <td>15.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-08</th>\n",
       "      <td>17.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-09</th>\n",
       "      <td>21.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-10</th>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Temp\n",
       "Date            \n",
       "1981-01-01  20.7\n",
       "1981-01-02  17.9\n",
       "1981-01-03  18.8\n",
       "1981-01-04  14.6\n",
       "1981-01-05  15.8\n",
       "1981-01-06  15.8\n",
       "1981-01-07  15.8\n",
       "1981-01-08  17.4\n",
       "1981-01-09  21.8\n",
       "1981-01-10  20.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_data = pd.read_csv('./data/daily-minimum-temperatures.csv',header=0, index_col=0)\n",
    "temp_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Scale the training and test data set using MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_length = 50\n",
    "train, test = temp_data[:-test_length], temp_data[-test_length:]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "# Scaling the train and test data using minmaxscaler\n",
    "train_scaled = scaler.fit_transform(train)\n",
    "test_scaled = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Build sequence-to-sequence data. For example, the length of the input and target sequence is 4 time steps, i.e.:\n",
    "\n",
    "        The raw data: [1,2,3,4,5,6,7,8,9,10,...]\n",
    "\n",
    "                              Prediction\n",
    "        Input Sequence      -------------->      Target Sequence\n",
    "          [1,2,3,4]                                [5,6,7,8]\n",
    "          [2,3,4,5]                                [6,7,8,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def seq_2_seq(data, length_x, length_y):\n",
    "    '''\n",
    "    convert the input data to seq-to-seq samples for training rnn model.\n",
    "    \n",
    "    Inputs:\n",
    "    data: pd.DataFrame, the input temperature data set.\n",
    "    length_x: The length of the input sequence\n",
    "    length_y: The length of the output sequence\n",
    "    \n",
    "    Outputs:\n",
    "    X: Input sequence, the size is: [number of samples, length_x, features]\n",
    "    y: Output sequence, the size is: [number of samples, length_y, features]\n",
    "    '''\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(data) - length_x - length_y):\n",
    "        X.append(data[i:i+length_x])\n",
    "        y.append(data[i+length_x:i+length_x+length_y])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input sequence size:  (num_sample, 7, 1)\n",
    "# Target sequence size: (num_sample, 1, 1)\n",
    "train_X, train_y = seq_2_seq(train_scaled, 7, 1)\n",
    "test_X, test_y = seq_2_seq(test_scaled, 7, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3592, 7, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that the NaN in the last rows should be dropped if you use pd.DataFrame.shift().\n",
    "print(np.any(np.isnan(train_X)))\n",
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3592, 1, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that the NaN in the last rows should be dropped if you use pd.DataFrame.shift().\n",
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 7, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that the NaN in the last rows should be dropped if you use pd.DataFrame.shift().\n",
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 1, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that the NaN in the last rows should be dropped if you use pd.DataFrame.shift().\n",
    "test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the sequences to tensor (dtype=torch.float32) with a size (num_sample, sequence_length, feature_size). \n",
    "train_X = torch.tensor(train_X, dtype=torch.float32)\n",
    "train_y = torch.tensor(train_y, dtype=torch.float32)\n",
    "test_X = torch.tensor(test_X, dtype=torch.float32)\n",
    "test_y = torch.tensor(test_y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** Complete the class of plain Vanilla Recurrent Neural Networks.\n",
    "\n",
    "The model contains one [RNN Layer](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html), which is followed by a Linear layer.\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "- The size of input data: ($L,N,H_{in}$), or ($N,L,H_{in}$) when batch_first=True.\n",
    "\n",
    "  $N$ = batch size\n",
    "  \n",
    "  $L$ = sequence length\n",
    "  \n",
    "  $H_{in}$ = input size\n",
    "  \n",
    "- The size of the connected linear layer: ($H_{rnn-out}, H_{out}$)\n",
    "    \n",
    "  $H_{rnn-out}$ = hidden size of the rnn layer\n",
    "  \n",
    "  $H_{out}$ = output size of the linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rnn(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size=1, output_size=1, hidden_size=32, num_layers=1, dropout=0.2,\n",
    "                 seq_length_in = 7, seq_length_out = 1, batch_first=True, \n",
    "                 activation_function='relu', random_init=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        '''\n",
    "        Initialize the hyperparameters and the layers of the network, including rnn layers and a linear layer.\n",
    "        input_size: int, dimension of the input features at each time step\n",
    "        output_size: int, dimension of the target at each time step\n",
    "        hidden_size: number of neuros in rnn layers\n",
    "        num_layers: number of rnn layers\n",
    "        dropout: dropout rate in rnn layers\n",
    "        seq_length_in: the sequence length of input data\n",
    "        seq_length_out: the sequence length of output (target) data\n",
    "        batch_first: boolean, if true, the size of the input data should be: \n",
    "                     [batch_size (number of samples), sequence length, features(or targets)]\n",
    "        activation_function: str, activation function in rnn layers\n",
    "        random_init: boolean, if true, the initial hidden states of the rnn layers are sampled from a normal distribution;\n",
    "                     Otherwise, it is set to zero. \n",
    "        '''\n",
    "        self.recurrent = torch.nn.RNN(input_size, hidden_size, num_layers, nonlinearity=activation_function,\n",
    "                                                  batch_first=batch_first, dropout=dropout)\n",
    "        self.linear = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.batch_first = batch_first\n",
    "        self.random_init = random_init\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_length_out = seq_length_out\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: torch.tensor, the input\n",
    "        \n",
    "        Feedforward propagation of the input to the output.\n",
    "        '''\n",
    "\n",
    "        h_0 = None\n",
    "\n",
    "        if self.random_init:\n",
    "            #(D*num_layers,N,Hout​)\n",
    "            h_0 = torch.randn(self.num_layers, x.shape[0] if self.batch_first else x.shape[1], self.hidden_size)\n",
    "        \n",
    "        # feedforward propagation of the input to the output   \n",
    "        out_rec, _ = self.recurrent(x, h_0) \n",
    "        output = self.linear(out_rec)[:,-self.seq_length_out,:].reshape(x.shape[0], self.seq_length_out, self.output_size) if self.batch_first else self.linear(out_rec)[-self.seq_length_out,:,:].reshape( self.seq_length_out, x.shape[0], self.output_size)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/TSDM-env/lib/python3.9/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rnn(\n",
       "  (recurrent): RNN(1, 32, batch_first=True, dropout=0.2)\n",
       "  (linear): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = rnn()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e)** Complete the function **training_process()** and train the rnn model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# The hyperparameters\n",
    "epochs = 256\n",
    "hidden_size = 32\n",
    "num_layers = 2\n",
    "input_size = train_X.shape[2]\n",
    "seq_length_in = train_X.shape[1]\n",
    "output_size = train_y.shape[2]\n",
    "print(output_size)\n",
    "seq_length_out = train_y.shape[1]\n",
    "activation_function = 'relu'\n",
    "dropout_rate = 0.2\n",
    "batch_size = 128\n",
    "early_stop_patience = 30\n",
    "batch_first = True\n",
    "random_init = False\n",
    "\n",
    "# Dataloader\n",
    "dataset = TensorDataset(train_X, train_y)\n",
    "loader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "model = rnn(input_size=input_size, output_size=output_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "            dropout=dropout_rate, seq_length_in = seq_length_in, seq_length_out = seq_length_out,\n",
    "            batch_first=batch_first, activation_function=activation_function, random_init=random_init)\n",
    "\n",
    "# Define the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_process(model, epochs, loader, optimizer, test_X_tensor, test_y_tensor, early_stop_patience):\n",
    "    '''\n",
    "    The training process is monitored by early stopping\n",
    "    \n",
    "    Inputs:\n",
    "    model: the neural network object of the given class\n",
    "    epochs: training iterations\n",
    "    loader: data loader for mini-batch training\n",
    "    optimizer: the given optimizer\n",
    "    test_X_tensor: torch.tensor, the input sequneces of test data\n",
    "    test_y_tensor: torcg.tensor, the target sequences of test data\n",
    "    early_stop_patience: int, the epochs for stopping the training\n",
    "    \n",
    "    Outputs:\n",
    "    best_model: the model with the lowest test error. Use deepcopy() to copy the best one.\n",
    "    train_losses: training losses\n",
    "    test_losses: test losses\n",
    "    '''\n",
    "    train_losses = [] # save the average training loss of each iteration\n",
    "    test_losses = [] # save the average test loss of each iteration\n",
    "    best_model = deepcopy(model)\n",
    "    \n",
    "    best_epoch = 0 # the epoch of the lowest test loss\n",
    "    lowest_loss = sys.float_info.max # the current lowest test loss\n",
    "    stop_epochs = 0\n",
    "    \n",
    "    # Define the MSE loss function\n",
    "    loss = torch.nn.MSELoss()\n",
    "\n",
    "    # Training process\n",
    "    for epoch in range(epochs):\n",
    "        # set training mode\n",
    "        model.train()\n",
    "        running_losses = 0.0\n",
    "\n",
    "        # Implement the training loop using mini-batch \n",
    "        for idx, batch in enumerate(loader):\n",
    "            optimizer.zero_grad()\n",
    "            x,y = batch\n",
    "            output = model(x)\n",
    "            loss_value = loss(output, y)\n",
    "            running_losses += loss_value.item()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "        train_losses.append(running_losses / (idx+1))\n",
    "\n",
    "\n",
    "        # compute test error in each epoch and add it to test_losses\n",
    "        # Do the same for training loss as well\n",
    "        # print the test error in each 10 epochs\n",
    "        model.eval()\n",
    "        output = model(test_X_tensor)\n",
    "        loss_value = loss(output, test_y_tensor).item()\n",
    "        test_losses.append(loss_value)\n",
    "        if(epoch % 10 == 0):\n",
    "            print(epoch, loss_value, f\"minloss {lowest_loss}\")\n",
    "        \n",
    "        # Implment early stopping\n",
    "        # Update the lowest loss, when the newest test loss is smaller.\n",
    "        # Otherwise stop_epochs += 1\n",
    "        # Stop learning and return the lowest test loss, when the stop_epochs is greater than or equal to\n",
    "        # the early_stop_patience\n",
    "        if(loss_value < lowest_loss):\n",
    "            lowest_loss = loss_value\n",
    "            stop_epochs = 0\n",
    "            best_epoch = epoch\n",
    "            best_model = deepcopy(model)\n",
    "        else:\n",
    "            stop_epochs += 1\n",
    "            if(stop_epochs > early_stop_patience):\n",
    "               break\n",
    "    return best_model, train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.029922571033239365 minloss 1.7976931348623157e+308\n",
      "10 0.009289445355534554 minloss 0.009318319149315357\n",
      "20 0.00931129977107048 minloss 0.008764375001192093\n",
      "30 0.008671422488987446 minloss 0.008538167923688889\n",
      "40 0.008677931502461433 minloss 0.00838476326316595\n",
      "50 0.008935026824474335 minloss 0.007990214973688126\n",
      "60 0.008528634905815125 minloss 0.0077445548959076405\n",
      "70 0.00842626579105854 minloss 0.0077445548959076405\n",
      "80 0.008327770046889782 minloss 0.007727807387709618\n",
      "90 0.0077285682782530785 minloss 0.007727807387709618\n",
      "100 0.008424029685556889 minloss 0.007727807387709618\n"
     ]
    }
   ],
   "source": [
    "model, train_losses, test_losses = training_process(model, epochs, loader, optimizer, \n",
    "                                                    test_X, test_y, early_stop_patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2a1f7b130>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the learning curve\n",
    "plt.plot(train_losses, label='train loss')\n",
    "plt.plot(test_losses, label='test loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the comparison between measurements and predictions of the training data\n",
    "### Your Code Here ###\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the comparison between measurements and predictions of the test data\n",
    "### Your Code Here ###\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Wind Power Generation Forecasts using LSTM (Bonus)\n",
    "\n",
    "In this exercise you need to implement an LSTM model to forecast wind power generation in the specific period based on the given 7 weather features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Import wind farm time-series dataset **wf.hdf5** in the data folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "HDF5ExtError",
     "evalue": "HDF5 error back trace\n\n  File \"H5F.c\", line 620, in H5Fopen\n    unable to open file\n  File \"H5VLcallback.c\", line 3502, in H5VL_file_open\n    failed to iterate over available VOL connector plugins\n  File \"H5PLpath.c\", line 579, in H5PL__path_table_iterate\n    can't iterate over plugins in plugin path '(null)'\n  File \"H5PLpath.c\", line 620, in H5PL__path_table_iterate_process_path\n    can't open directory: /opt/homebrew/anaconda3/envs/TSDM-env/lib/hdf5/plugin\n  File \"H5VLcallback.c\", line 3351, in H5VL__file_open\n    open failed\n  File \"H5VLnative_file.c\", line 97, in H5VL__native_file_open\n    unable to open file\n  File \"H5Fint.c\", line 1990, in H5F_open\n    unable to read superblock\n  File \"H5Fsuper.c\", line 617, in H5F__super_read\n    truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048\n\nEnd of HDF5 error back trace\n\nUnable to open/create file './data/wf.hdf5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHDF5ExtError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m datastore \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHDFStore\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data/wf.hdf5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m datastore\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwindfarm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/TSDM-env/lib/python3.9/site-packages/pandas/io/pytables.py:578\u001b[0m, in \u001b[0;36mHDFStore.__init__\u001b[0;34m(self, path, mode, complevel, complib, fletcher32, **kwargs)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fletcher32 \u001b[38;5;241m=\u001b[39m fletcher32\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 578\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/TSDM-env/lib/python3.9/site-packages/pandas/io/pytables.py:737\u001b[0m, in \u001b[0;36mHDFStore.open\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    731\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    732\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot open HDF5 file, which is already opened, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    733\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meven in read-only mode.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    734\u001b[0m     )\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m--> 737\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43mtables\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/TSDM-env/lib/python3.9/site-packages/tables/file.py:300\u001b[0m, in \u001b[0;36mopen_file\u001b[0;34m(filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    296\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is already opened.  Please \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclose it before reopening in write mode.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m filename)\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# Finally, create the File instance, and return it\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_uep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/TSDM-env/lib/python3.9/site-packages/tables/file.py:750\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams \u001b[38;5;241m=\u001b[39m params\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Now, it is time to initialize the File extension\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_g_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;66;03m# Check filters and set PyTables format version for new files.\u001b[39;00m\n\u001b[1;32m    753\u001b[0m new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_v_new\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/TSDM-env/lib/python3.9/site-packages/tables/hdf5extension.pyx:484\u001b[0m, in \u001b[0;36mtables.hdf5extension.File._g_new\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mHDF5ExtError\u001b[0m: HDF5 error back trace\n\n  File \"H5F.c\", line 620, in H5Fopen\n    unable to open file\n  File \"H5VLcallback.c\", line 3502, in H5VL_file_open\n    failed to iterate over available VOL connector plugins\n  File \"H5PLpath.c\", line 579, in H5PL__path_table_iterate\n    can't iterate over plugins in plugin path '(null)'\n  File \"H5PLpath.c\", line 620, in H5PL__path_table_iterate_process_path\n    can't open directory: /opt/homebrew/anaconda3/envs/TSDM-env/lib/hdf5/plugin\n  File \"H5VLcallback.c\", line 3351, in H5VL__file_open\n    open failed\n  File \"H5VLnative_file.c\", line 97, in H5VL__native_file_open\n    unable to open file\n  File \"H5Fint.c\", line 1990, in H5F_open\n    unable to read superblock\n  File \"H5Fsuper.c\", line 617, in H5F__super_read\n    truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048\n\nEnd of HDF5 error back trace\n\nUnable to open/create file './data/wf.hdf5'"
     ]
    }
   ],
   "source": [
    "datastore = pd.HDFStore('./data/wf.hdf5')\n",
    "data = datastore.get('windfarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The windfarm dataset contains **hourly averaged wind power generation time series** for two consecutive years and **the corresponding day-ahead meteorological forecasts** using the European Centre for Medium-Range Weather Forecasts (ECMWF) weather model.\n",
    "\n",
    "The data set contains the following data items:\n",
    "\n",
    "- Time Stamp of the measurement\n",
    "- Forecasting Time, Time between the creation of the forecast to the forecasted point in time\n",
    "- Air Pressure of the measurement\n",
    "- Air Temperature of the measurement\n",
    "- Humidity\n",
    "- Wind Speed in 100m height\n",
    "- Wind Speed in 10m height\n",
    "- Wind Direction (zonal) in 100m height\n",
    "- Wind Direction (meridional) in 100m height\n",
    "- Wind Power Generation of the wind farm\n",
    "\n",
    "**The power generation time series are normalized** with the respective nominal capacity of the wind farm in order to enable a scale-free comparison and to mask the original characteristics of the wind farm. \n",
    "\n",
    "Additionally, **all weather situations are normalized in the range \\[0..1\\]**. \n",
    "\n",
    "The data set is pre-filtered to **discard any period of time longer than 24h in which no energy has been produced**, as this is an indicator of a wind farm malfunction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Visualize the weather features and the power generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data.columns[2:]:\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(data[col].values[:1000])\n",
    "    plt.title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Convert the wind farm time series to sequence-to-sequence training samples.\n",
    "        \n",
    "**Hints**:\n",
    "\n",
    "- The dimension of input sequences should be $[N, 6, 7]$, where $N$ indicates the number of samples, 6 indicates the time steps of the input sequence, and 7 indicates the dimension of weather features.\n",
    "\n",
    "- The dimension of an output sequnece should be $[N, 6, 1]$, where $N$ indicates the number of samples, 6 indicates the time steps of the target sequence, and 1 indicates the dimension of the target, i.e., wind power generation.\n",
    "\n",
    "- The **discarded periods** should be considered while building sequence-to-squence samples. If the difference between the two successive time indices is greater than 1 hour, **print** and **store** the two successive time indices.\n",
    "\n",
    "- You can compare your cleaned sequence data to the given data in 'clean_wf.hdf5'.\n",
    "\n",
    "- If you have no idea how to implement the function **seq_2_seq**, you can directly use 'clean_wf.hdf5' to finish the following tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = data[:12000], data[12000:]\n",
    "seq_length = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the discarded periods in training dataset \n",
    "discarded_periods_train = []\n",
    "discarded_periods_train.append(train.index[0])\n",
    "print('discarded periods in training dataset:\\n')\n",
    "for idx_1, idx_0 in zip(train.index[1:], train.index[:-1]):\n",
    "    # if the difference between two successive time indices is greater than 1 hour, print and store the time indices.     \n",
    "    ### Your Code Here ###\n",
    "    pass\n",
    "    \n",
    "discarded_periods_train.append(train.index[-1])\n",
    "\n",
    "\n",
    "# Print the discarded periods in test dataset \n",
    "discarded_periods_test = []\n",
    "discarded_periods_test.append(test.index[0])\n",
    "print('\\ndiscarded periods in test dataset:\\n')\n",
    "for idx_1, idx_0 in zip(test.index[1:], test.index[:-1]):\n",
    "    # if the difference between two successive time indices is greater than 1 hour, print and store the time indices.\n",
    "    ### Your Code Here ###\n",
    "    pass\n",
    "    \n",
    "discarded_periods_test.append(test.index[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_2_seq(data, discarded_periods, seq_length):\n",
    "    '''\n",
    "    Clean the discareded periods and convert the data to sequence-to-sequence data\n",
    "    \n",
    "    Inputs:\n",
    "    data: pd.DataFrame, the raw data\n",
    "    discareded_periods: list, which contains the interrupted time indices.\n",
    "    seq_length: int, the sequence length of generated input and output data.\n",
    "    \n",
    "    '''\n",
    "    data_seq2seq = pd.DataFrame([],columns=data.columns)\n",
    "    for i in range(0,len(discarded_periods), 2):\n",
    "        data_selected = data.loc[discarded_periods[i]:discarded_periods[i+1]]\n",
    "        if data_selected.shape[0] < seq_length:\n",
    "            continue\n",
    "        \n",
    "        for idx in range(data_selected.shape[0]-seq_length):\n",
    "            # concatenate the successive samples with the given length to the data_seq2seq\n",
    "            ### Your Code Here ###\n",
    "            pass\n",
    " \n",
    "    return data_seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = seq_2_seq(train, discarded_periods_train, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq.iloc[:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = seq_2_seq(test, discarded_periods_test, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq.iloc[:14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can compare your cleaned sequence data to the given data in 'clean_wf.hdf5'.\n",
    "\n",
    "If you have no idea how to implement the function **seq_2_seq**, you can directly use 'clean_wf.hdf5' to finish the following tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the clean_wd,hdf5 if you can not implement the function seq_2_seq\n",
    "# Otherwise, use the function seq_2_seq to generate seq-seq samples for training and test.\n",
    "### Your Code Here ###\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train_seq/test_seq to train_X and train_y/test_X and test_y with the size \n",
    "# [number of samples, sequence length, features(target)], and convert to torch.tensor (dtype=torch.float32).\n",
    "### Your Code Here ###\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'size of train_X_tensor: {train_X_tensor.shape}')\n",
    "train_X_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'size of test_X_tensor: {test_X_tensor.shape}')\n",
    "test_X_tensor[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** Complete the class of LSTM models.\n",
    "\n",
    "The model contains [LSTM Layer](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html), which is followed by a Linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size=1, output_size=1, hidden_size=32, num_layers=1, dropout=0.2,\n",
    "                 seq_length_in = 7, seq_length_out = 1, batch_first=True, random_init=True):\n",
    "        super().__init__()\n",
    "        '''\n",
    "        Initialize the hyperparameters and the layers of the network, including lstm layers and a linear layer.\n",
    "        input_size: int, dimension of the input features at each time step\n",
    "        output_size: int, dimension of the target at each time step\n",
    "        hidden_size: number of neuros in lstm layers\n",
    "        num_layers: number of lstm layers\n",
    "        dropout: dropout rate in lstm layers\n",
    "        seq_length_in: the sequence length of input data\n",
    "        seq_length_out: the sequence length of output (target) data\n",
    "        batch_first: boolean, if true, the size of the input data should be: \n",
    "                     [batch_size (number of samples), sequence length, features(or targets)]\n",
    "        random_init: boolean, if true, the initial hidden states and cell states of the lstm layers are sampled from a normal \n",
    "                     distribution; Otherwise, it is set to zero. \n",
    "        '''\n",
    "        ### Your Code Here ###\n",
    "        pass\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: torch.tensor, the input\n",
    "        \n",
    "        Feedforward propagation of the input to the output.\n",
    "        '''\n",
    "        \n",
    "        if self.batch_first:\n",
    "            # get the batch size from the input x to create the initial hidden states h0 and cell states c0\n",
    "            ### Your Code Here ###\n",
    "            pass\n",
    "        else:\n",
    "            ### Your Code Here ###\n",
    "            pass\n",
    "        if self.random_init:\n",
    "            ### Your Code Here ###\n",
    "            pass\n",
    "        else:\n",
    "            ### Your Code Here ###\n",
    "            pass\n",
    "        \n",
    "        # feedforward propagation of the input to the output \n",
    "        ### Your Code Here ###\n",
    "        pass\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e)** Declare an lstm model and train it using the above function **training_process()**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The hyperparameters\n",
    "epochs = 256\n",
    "hidden_size = 32\n",
    "num_layers = 2\n",
    "input_size = train_X_tensor.shape[2]\n",
    "seq_length_in = train_X_tensor.shape[1]\n",
    "output_size = train_y_tensor.shape[2]\n",
    "seq_length_out = train_y_tensor.shape[1]\n",
    "dropout_rate = 0.2\n",
    "batch_size = 128\n",
    "early_stop_patience = 30\n",
    "batch_first = True\n",
    "random_init = True\n",
    "\n",
    "# Dataloader\n",
    "dataset = TensorDataset(train_X_tensor, train_y_tensor)\n",
    "loader = DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = lstm(input_size=input_size, output_size=output_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                  dropout=dropout_rate, seq_length_in = seq_length_in, seq_length_out = seq_length_out,\n",
    "                  batch_first=batch_first, random_init=random_init)\n",
    "\n",
    "\n",
    "\n",
    "# Define loss function\n",
    "### Your Code Here ###\n",
    "pass\n",
    "\n",
    "# Define optimizer\n",
    "### Your Code Here ###\n",
    "pass\n",
    "\n",
    "# Train the lstm model using the function training_process()\n",
    "lstm_model, train_losses, test_losses = training_process(lstm_model, epochs, loader, optimizer_lstm, \n",
    "                                                         test_X_tensor, test_y_tensor, early_stop_patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the comparison between measurements and predictions of the test data at the first time step, \n",
    "# i.e, plt.plot(test_y_tensor[:,0,:])\n",
    "### Your Code Here ###\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the error of each time step. \n",
    "### Your Code Here ###\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
