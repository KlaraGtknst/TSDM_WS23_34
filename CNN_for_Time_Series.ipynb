{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "Today we will show the Process of classificating Time-series starting with simple TS data using a sliding window approach\n",
    "* First we show the preprocessing of a timeseries given as a continouus vector to a usable Dataloader\n",
    "* Next we will show how to train a ConvNet using PyTorch on a Time-series classification Task\n",
    "* We will show how to optimize parameters like Kernelsize, and number of kernels (i.e. number of features) etc. using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, ConcatDataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Classification Using CNN\n",
    "\n",
    "The Data consist of three types of time series\n",
    "* Step function\n",
    "* Sine Wave\n",
    "* Sawtooth Wave\n",
    "\n",
    "To make it more difficult for the classification, a strong noise is added to the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ts_steps(n, m, noise):\n",
    "    \"\"\"Sample a simple random two-class time series\"\"\"\n",
    "    s = np.random.random((n)) * noise - noise / 2\n",
    "    k = int(len(s) / m)\n",
    "    for i in range(m):\n",
    "        if i % 2 == 0:\n",
    "            continue\n",
    "        s[k * i:k * (i + 1)] += +1\n",
    "    return s\n",
    "\n",
    "def get_ts_wave(n, m, noise):\n",
    "    \"\"\"Sample a simple random two-class time series\"\"\"\n",
    "    x = np.sin(np.linspace(-np.pi, m*np.pi, n))\n",
    "    s = np.random.random((n)) * noise - noise / 2\n",
    "    return x+s\n",
    "\n",
    "def get_Sawtooth_wave(n, m, noise):\n",
    "    \"\"\"Sample a simple random two-class time series\"\"\"\n",
    "    s = np.random.random((n)) * noise - noise / 2 - 1\n",
    "    k = int(len(s) / m)\n",
    "    lin_func = lambda x: 2/k * x\n",
    "    lin = lin_func(np.arange(k))\n",
    "    for i in range(m):\n",
    "        if i % 2 == 0:\n",
    "            continue\n",
    "        s[k * i:k * (i + 1)] += 1*lin\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">First we generate long Timeseries with n = 50000 datapoints each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000 \n",
    "m = 500\n",
    "noise = .1\n",
    "\n",
    "X_steps = get_ts_steps(n, m, noise)\n",
    "X_wave = get_ts_wave(n, m, noise)\n",
    "X_sawtooth = get_Sawtooth_wave(n, m, noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** Plot first 200 points of each Timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "####################\n",
    "# Your Code Here   #\n",
    "####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)** Create a Dataloader for Training\n",
    "\n",
    ">A dataloader is necessary for loading a consistent representation of values in the input space for the training, evaluation and test process. \n",
    "The idea is to train a model without dependecy of the length of the whole dataset. \n",
    "In this case we have 3 synthetic time series with 50000 data points. Fitting all the data at once into a model would lead to high computational requirements. \n",
    "\n",
    ">We will use a sliding window approach to split the data into smaller pieces and the length of the segments extracted here should be considered as a hyperparameter value. \n",
    "\n",
    "[Tutorial](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) gives an overview over the process. The Dataset class must implement three functions: \n",
    "` __init__`\n",
    "` __len__`\n",
    "` __getitem__`\n",
    "\n",
    "for our case we need the additional function:\n",
    "`split_sequence` to split the long time series into segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesData(Dataset):\n",
    "    def __init__(self, X, y, sw):\n",
    "        \n",
    "        self.X = X # Time Series data\n",
    "        self.y = y # class\n",
    "        self.sw = sw # window size of the sliding window\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        # The __len__ function returns the number of samples in our dataset\n",
    "        # hint: you need to take into account the window size s\n",
    "        \n",
    "        ####################\n",
    "    # Your Code Here   #\n",
    "    ####################\n",
    "        \n",
    "    def split_sequence(self, X, y, index):\n",
    "        \n",
    "        # The split_sequence class returns the time series vector from the given index \n",
    "        # up to the given index plus the window size \n",
    "        # Additionaly it gives the realted class\n",
    "        \n",
    "        ####################\n",
    "    # Your Code Here   #\n",
    "    ####################\n",
    "    \n",
    "    def __getitem__(self, idx):  \n",
    "    \n",
    "        # The __getitem__ function returns a sample from the dataset at the given index idx\n",
    "        # It should return it as a torch.Tensor class\n",
    "        \n",
    "        ####################\n",
    "    # Your Code Here   #\n",
    "    ####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)** Creating Dataloader for training, testing and validation\n",
    "\n",
    "* Create Datasets for each Time series seperatly\n",
    "* Concat the datasets using [ConcatDataset](https://pytorch.org/docs/stable/data.html)\n",
    "* Split the DataSet using [random_split](https://pytorch.org/docs/stable/data.html)\n",
    "* Create Dataloader using [DataLoader](https://pytorch.org/docs/stable/data.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = 50\n",
    "\n",
    "steps_DS = TimeSeriesData(X_steps, y=np.ones(len(X_steps))*0, sw = sw) # Dataset for class 0, step_function\n",
    "wave_DS = TimeSeriesData(X_wave, y=np.ones(len(X_steps))*1, sw = sw)   # Dataset for class 1, wave_function\n",
    "sawtooth_DS = TimeSeriesData(X_sawtooth, y=np.ones(len(X_steps))*2, sw = sw) # Dataset for class 2, sawtooth-function\n",
    "\n",
    "# concat the datasets\n",
    "####################\n",
    "# Your Code Here   #\n",
    "####################\n",
    "\n",
    "# split the dataset in trainset, valset and testset with a 70%, 15%, 15% split\n",
    "####################\n",
    "# Your Code Here   #\n",
    "####################\n",
    "\n",
    "# Create Dataloader for training, testing an valitation\n",
    "batch_size = 13\n",
    "####################\n",
    "# Your Code Here   #\n",
    "####################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_, y_ = next(iter(loader_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4)** Implement a CNN architecture\n",
    "\n",
    ">Use one 1D Convolutional Layer with Relu activation and max pooling (kernel = 2). After the convolutional Layer one Linear layer need to be applied. Lastly the classification should be realized by a Sofmax Function <br>\n",
    "<br>\n",
    "hint: you need to take into account the different output dimensions, according to convolution and pooling outup <br>\n",
    "<br>\n",
    "Convolutional output ([link](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html)): <br>\n",
    "<br>\n",
    "$$L_{out} = \\left[\\frac{L_{in} + 2\\cdot padding - dilation\\cdot(kernel\\_size - 1) - 1}{stride} + 1\\right]$$\n",
    "<br>\n",
    "<br>\n",
    "padding = 0 <br>\n",
    "stride = 1 <br>\n",
    "dilation = 1 <br>\n",
    "<br>\n",
    "pooling output ([link](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool1d.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, n_feature, kernel_size, output_size):\n",
    "        super(CNN, self).__init__()\n",
    "        ####################\n",
    "    # Your Code Here   #\n",
    "    ####################\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ####################\n",
    "    # Your Code Here   #\n",
    "    ####################\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5)** Implement a training and testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer, loader_train):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader_train):\n",
    "        # Here you need to implement a training process, which uses the given optimizer \n",
    "        # claculates the loss and backprobs through the network\n",
    "        # hint: take a look at previous implementations in other excersices\n",
    "        ####################\n",
    "    # Your Code Here   #\n",
    "    ####################\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6)** Implement a grid search optimization for the Numer of kernels ([`out_channels`](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html)) and the kernel size\n",
    "\n",
    ">The next step is to implement a short grid search for the Number of features and the kernel size, use an algorithm like in excercise 10, optimization. You can use the previously defined training and testing functions to make you code look more clear\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_feature = [1, 2, 3, 4, 5, 6]\n",
    "kernel_size = [1, 3, 5, 7, 9]\n",
    "epochs = 3\n",
    "results_acc_list = []\n",
    "results_loss_list = []\n",
    "\n",
    "for feature in n_feature:\n",
    "    results_acc = []\n",
    "    results_loss = []\n",
    "    for kernel in kernel_size:\n",
    "\n",
    "        ####################\n",
    "    # Your Code Here   #\n",
    "    ####################\n",
    "        \n",
    "    results_acc_list.append(results_acc)\n",
    "    results_loss_list.append(results_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7)** Visualize the optimization results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "#ax.view_init(30, 60)\n",
    "X, Y = np.meshgrid(n_feature, kernel_size)\n",
    "ax.plot_surface(X, Y, np.vstack(results_loss_list).transpose(), cmap=\"plasma\")\n",
    "ax.set_xlabel('n_feature', fontsize=15, rotation=60)\n",
    "ax.set_ylabel('kernel_size', fontsize=15, rotation=60)\n",
    "plt.title('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "#ax.view_init(30, 225)\n",
    "X, Y = np.meshgrid(n_feature, kernel_size)\n",
    "ax.plot_surface(X, Y, np.vstack(results_acc_list).transpose(), cmap=\"plasma\")\n",
    "ax.set_xlabel('n_feature', fontsize=15, rotation=60)\n",
    "ax.set_ylabel('kernel_size', fontsize=15, rotation=60)\n",
    "plt.title('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8)** Final Results: Use the test Set to evaluate the Model with the best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Your Code Here   #\n",
    "####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9)** What other hyperparameter can be optimized? \n",
    "\n",
    ">Discussion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
